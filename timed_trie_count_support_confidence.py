# -*- coding: utf-8 -*-
"""timed_trie_2_response_alternate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1siixg1gbp-Jv0SZe0cbkvlIohQ6n09av

# Timed Trie (response/alternate/... analysis)
This notebook is for application of timmed trie on (response/alternate/...) trace
"""

# !pip install -q rstr;
# !pip install -q pyvis;
# !pip install -q graphviz seaborn tabulate
# !apt-get -qq install graphviz graphviz-dev -y && pip install -q pygraphviz;

import matplotlib.pyplot as plt
from graphviz import Digraph
import numpy as np
import pandas as pd

import rstr
import random
import sys
import seaborn
import statistics
import difflib
import time
from typing import Tuple
from itertools import accumulate
from operator import add
import copy
import re
from tabulate import tabulate
import numpy as np

"""# Import file

# Utility functions
"""
def getNodeId(node_id: str):
    if len(node_id.split("_")) > 1:
        return str(int(node_id.split("_")[1]) - 1);
    return "-1"

def getStateId(node_id: str):
    return "q_" + getNodeId((node_id));

def getTimeShift(time):
    time_new = [0]
    # Complex but short
    # time_shift = [0]
    # time_shift.extend(time[0:len(time)-1])
    # time_new = list(0 if i == 0 else time[i]-time_shift[i] for i in list(range(0, len(time))))

    # Simple but long
    for i in range(1, len(time)):
        time_diff = max(0, time[i] - time[i - 1])
        time_new.append((time_diff))

    contains_negative = [True if _ele < 0 else False for _ele in time_new]
    if True in contains_negative:
        print("This list contains negative value ", time)
        print("Processed list ", time_new)

    return time_new

def doesStringContains(str="", whichWord=""):
    return str.find(whichWord) > -1

"""# Mine for properties

## Trace import
"""

"""#### Parameter"""

ENABLE_IQR = True
STD_VAL = 0.675  # Uses only if ENABLE_IQR is False
K = 5 # Depth
P_THRESHOLD = 0.45 # Probability threshold to extract dominant properties

VARIABILITY_WINDOW_LENGTH = 5  # for synthetic trace generation

ENABLE_DEBUG = False; #Enables debug
ENABLE_STRICT_TEMPORAL_MATCH = True; # Src -> Des must contain
PLOT_PROBABILITY_ALPBHABET_MAX_LENGTH = 15; # Plot fails for large alpbhabet size

SYNTHETIC_TRACE_PATH = "/home/pradeep/UBC/Projects/Data-Mining/TTT - A_star/codebase/timedStateTrie/Dataset/synthetic_trace2.csv"
HEXCOPTER_TRACE_PATH = "/home/pradeep/UBC/Projects/QRE/DataSet/Hexcopter from QNX  RTOS Tracelogger/hexacopter-20210327T224956Z-001/hexacopter/csvs/hil-RF-Baseline-Clean2.csv"

"""#### Import trace"""

def getSynthetic():
    global SYNTHETIC_TRACE_PATH
    trace_df = pd.read_csv(SYNTHETIC_TRACE_PATH)

    time = list(map(int, list(trace_df.loc[:, "Time"].values)))
    event = list(trace_df.loc[:, "Event"].values)

    return (list(event), time)

def getHexcopterTrace():
    global HEXCOPTER_TRACE_PATH
    hil_RF_Baseline_Clean_df = pd.read_csv(HEXCOPTER_TRACE_PATH, nrows=10000)
    hil_RF_Baseline_Clean_df.sort_values(by=["time"], ascending=True, inplace=True)

    ignore_events = ["TIME", "0x00000029", "0x0000002d", "0x0000002e", "0x00000044", "0x00000049"]
    hil_RF_Baseline_Clean_df = hil_RF_Baseline_Clean_df[~hil_RF_Baseline_Clean_df.event.isin(ignore_events)]

    event = list(hil_RF_Baseline_Clean_df['event'])

    def removed_while_character(str, wild):
        _event = ''.join([_e if _e != wild else "_" for _e in list(str)])
        return _event

    # Remove wild charactes
    for _c in ["*","/","-"]:
        event = [removed_while_character(_ev, _c) for _ev in event]

    trace_time = getTimeShift(list(hil_RF_Baseline_Clean_df['time']))
    return (list(event), trace_time)

timed_trace = getSynthetic()
# timed_trace = getHexcopterTrace()
trace_size = len(timed_trace[0])
print("Trace Length ", trace_size)
print("Unique Events ", len(set(timed_trace[0])))

"""## Create Timed Trie

### Define
"""

global_node_count = 0

class TrieNode(object):
    def __init__(self, id: int, char: str):
        self.node_id = "Node_" + str(id)
        self.char = char
        self.children = []
        self.count = 1
        self.t_min = sys.maxsize
        self.t_max = 0
        self.t_mean = 0
        self.t_list = []
        self.t_var = 0
        self.dropped = 0
        self.prob = 0.0 # probability of occuring during transition
        self.prob_pattern = 0.0 #probability of pattern occuring
        self.tree_height = 0 # height of the tree considering the present node as root
        self.is_end = True


def traverseAndBuild(node: TrieNode, timedTrace: (list(), list()), pos: int):
    if pos >= len(timedTrace[0]):
        return

    global global_node_count
    node.is_end = False
    # print("timed_trace ", timed_trace)

    event, time = timedTrace[0][pos: pos + 1], timedTrace[1][pos: pos + 1]
    # print("event, time, pos ", event, time, pos)
    event, time = event[0], time[0]
    found = False
    doTimeCheck = True if pos < len(timedTrace[0]) - 1 else False

    for child in node.children:
        if child.char == event:  # check with character only
            if doTimeCheck == False or (doTimeCheck == True and time >= child.t_min and time <= child.t_max):
                found = True
                if doTimeCheck == False:
                    child.count += 1
                    child.t_min = min(child.t_min, time)
                    child.t_max = max(child.t_max, time)
                    child.t_list.append(time)
                traverseAndBuild(child, timedTrace, pos + 1)
                return

    if not found and doTimeCheck == False:  # only create for last element in the trace
        global_node_count += 1
        newNode = TrieNode(global_node_count, event)
        newNode.t_min = time
        newNode.t_max = time
        newNode.t_list.append(time)
        newNode.count = 1
        node.children.append(newNode)
        traverseAndBuild(newNode, timedTrace, pos + 1)


def evaluateProb(node: TrieNode, d: int, current_d: int):
    if current_d > d:
        return

    global trace_size
    # find pattern probability
    # height_of_currentTree = node.tree_height;
    # total_patterns = sum(trace_size-(k-1) for k in list(range(current_d+1, height_of_currentTree + 2))) # We need all possible pattern sizes from 1 -> current_d+1
    # total_patterns = sum(trace_size-(k-1) for k in list(range(current_d+1, height_of_currentTree + 2))) # We need all possible pattern sizes from 1 -> current_d+1
    node.prob_pattern = round(float(node.count / (trace_size-current_d+1)), 2)

    # find transition probability
    tot_count = 0 if len(node.children) > 0 else 1
    _inner_count_list = []

    for _child in node.children:
        tot_count += _child.count
        _inner_count_list.append((_child.count))

    # TODO: Look into this. This block is for fail-safe but the necessity of this should never happen
    if len(node.children) > 0 and tot_count == 0:
        tot_count = 1


    for _child in node.children:
        try:
            _child.prob = round(float(_child.count / tot_count), 2)
        except ZeroDivisionError:
            print("ZeroDivisionError ")
            print("node.children count:{0}  tot_count:{1}  count_list:{2}".format(str(len(node.children)), str(tot_count), _inner_count_list))
            raise ZeroDivisionError

    for _child in node.children:
        evaluateProb(_child, d, current_d + 1)

def evaluateHeightOfTree(node: TrieNode, d: int, current_d: int):
    max_child_height = 0;
    for _child in node.children:
        max_child_height = max(max_child_height, evaluateHeightOfTree(_child, d, current_d+1))

    node.tree_height = max_child_height + 1;
    return node.tree_height

def evaluateAtDepth(node: TrieNode, d: int, current_d: int):
    if d <= 1:
        return

    global STD_VAL, ENABLE_IQR
    if current_d < d - 1:
        for child in node.children:
            evaluateAtDepth(child, d, current_d + 1)
    else:
        node.t_mean = statistics.mean(node.t_list)
        if ENABLE_IQR:
            _var = np.percentile(node.t_list, [25, 75])
            node.t_min, node.t_max = node.t_mean - _var[0], node.t_mean + _var[1]
        else:
            node.t_var = statistics.pstdev(node.t_list)
            _var = STD_VAL * node.t_var
            node.t_min, node.t_max = node.t_mean - _var, node.t_mean + _var

        node.t_min, node.t_max = round(node.t_min), round(node.t_max)
        # print("node.t_min ", node.t_min, "  node.t_max  ", node.t_max)

        # Take inner quartile
        node.count = sum(ele >= node.t_min and ele <= node.t_max for ele in node.t_list)
        node.dropped = len(node.t_list) - node.count


def buildGraph(timed_trace, K: int = 3) -> TrieNode:
    global global_node_count
    global_node_count += 1

    root = TrieNode(global_node_count, "*")
    for k in list(range(1, K + 1)):
        # print("depth ------ ", k)
        for i in list(range(0, len(timed_trace[0]) + 1 - k)):
            sub_trace_event = timed_trace[0][i: i + k]
            sub_trace_time = timed_trace[1][i: i + k]
            sub_trace_time = getTimeShift(sub_trace_time)  # Get reset time shit
            # print("from ", str(i), " to ", str(i+k), " ", sub_trace_event)

            sub_trace = (sub_trace_event, sub_trace_time)
            # print(sub_trace)
            traverseAndBuild(root, sub_trace, 0)
        evaluateAtDepth(root, k, 0)

    evaluateHeightOfTree(root, k, 0)
    evaluateProb(root, k, 0)
    return root


"""### Perform Mining"""

start = time.time()

timed_trie = buildGraph(timed_trace, K)
time_diff = round(time.time() - start, 2)
print("Timed Trie build complete")
print("Timed Elapsed : ", str(time_diff), " sec")

timed_trie

"""# Generate Digraph for Timed Trie

#### Digraph visualizer
"""


def traverseAndCreaseDigraph(dot: Digraph, node: TrieNode, state_graph=False):
    parent_id = node.char if not state_graph else getStateId(node.node_id)

    if not node.children:
        dot.attr('node', shape="doublecircle")
    else:
        dot.attr('node', shape="circle")

    dot.node(node.node_id, parent_id)

    tot_count = 1
    for _child in node.children:
        tot_count += _child.count

    for child in node.children:

        prefix = "" if not state_graph else (child.char + " , ");
        edge_label = prefix + "[" + str(child.t_min) + "," + str(child.t_max) + "]";
        if state_graph:
            edge_label += " , " + str(child.prob) + ", " + str(child.prob_pattern)
        else:
            edge_label += " { Count : " + str(child.count) + " Dropped : " + str(child.dropped) + " }";

        child_id = child.char if not state_graph else getStateId(child.node_id)

        if not child.children:
            dot.attr('node', shape="doublecircle")
        else:
            dot.attr('node', shape="circle")

        dot.node(child.node_id, child_id)

        dot.edge(node.node_id, child.node_id, label=edge_label)
        traverseAndCreaseDigraph(dot, child, state_graph)


def visualizeTrie(timed_trie: TrieNode, state_graph=False):
    dot = Digraph(comment='Timed Trie', format='png')
    traverseAndCreaseDigraph(dot, timed_trie, state_graph)
    return dot

def renderTrie(timed_trie: TrieNode, diagram_name="timed_trie", state_graph=False):
    timed_trie_viz = visualizeTrie(timed_trie, state_graph)
    timed_trie_viz.render(diagram_name+'.gv', view=True)
    return timed_trie_viz

"""## Visualize"""

# renderTrie(timed_trie, "timed_trie_state_diagram", True)
# renderTrie(timed_trie, "timed_trie", False)

"""### Extract expression"""

extractedPatternList = []

def extractPattern(node: TrieNode, max_d: int, current_d: int, current_path: list = []):
    if current_d > max_d:
        return;

    global P_THRESHOLD

    currentPattern = ""

    for child in node.children:
        if child.prob >= P_THRESHOLD or current_d == 0:
            new_path = current_path + [child]
            extractedPatternList.append((copy.copy(new_path)))

            inner_patterns = extractPattern(child, max_d, current_d + 1, copy.copy(new_path))


start = time.time()
extractPattern(timed_trie, K, 0)
extractedPatternList = set(tuple(item) for item in extractedPatternList)

time_diff = round(time.time() - start, 2)
print("Dominant patterns extracted")
print("Timed Elapsed : ", str(time_diff), " sec")

# Find the number of matching patterns
def findMatchingPatterns(pattern_to_test):
    global timed_trace # the entire pattern
    count = 0

    current_pattern = []
    for ele in timed_trace[0]:
        if len(current_pattern) == len(pattern_to_test):
            current_pattern = current_pattern[1:]

        current_pattern.append(ele)

        # this needs to be done separately so that append and checking are divided and not mixed up
        if current_pattern == pattern_to_test:
            count += 1

    return count

# Find the support and confidence
def evaluateSupportAndConfidenceForList(pattern_to_test):
    global timed_trace
    count_for_exact_pattern = findMatchingPatterns(pattern_to_test);
    count_of_one_less_pattern = findMatchingPatterns(pattern_to_test[:-1])

    total_count = len(timed_trace[0])

    # print("count_for_exact_pattern-{0} count_of_one_less_pattern-{1} total_count-{2}".format(count_for_exact_pattern, count_of_one_less_pattern, total_count))

    support = count_for_exact_pattern / (total_count - (len(pattern_to_test[0]) - 1))
    confidence = 1 if len(pattern_to_test) == 1 else count_for_exact_pattern / count_of_one_less_pattern

    return [round(support, 5), round(confidence, 5)];

extractedPatternList_str_count = []

print("Possible Patterns alternate approach. (unique count):{0}  Threshold : {1}".format(str(len(extractedPatternList)), str(P_THRESHOLD)))
def printFromPatternList(patternList: list = [], patternList_str_count = [], returnPatternCount=False):

    for _ele in patternList:
        _currentPattern = ""
        _plain_pattern = []
        for idx, inner_ele in enumerate(_ele):
            if idx > 0:
                _currentPattern = _currentPattern + "[" + str(inner_ele.t_min) + "," + str(inner_ele.t_max) + "]"
            else:
                _currentPattern = _currentPattern + "[-INF," + str(inner_ele.t_max) + "]"

            _plain_pattern.append(inner_ele.char)
            _currentPattern += inner_ele.char + " "

        "### Calculate Support and Confidence (if-required)"
        support_conf = evaluateSupportAndConfidenceForList((_plain_pattern))

        if returnPatternCount == True:
            return [_currentPattern, inner_ele.count, support_conf[0], support_conf[1]] # so that the callee can make changes accordingly
        else:
            patternList_str_count.append([_currentPattern, inner_ele.count, support_conf[0], support_conf[1]])
        # print(_currentPattern)

printFromPatternList(extractedPatternList, extractedPatternList_str_count)
extractedPatternList_DF = pd.DataFrame(extractedPatternList_str_count, columns=["Pattern","Count", "Support", "Confidence"])
extractedPatternList_DF.sort_values(by=["Count"], ascending=False, inplace=True)

print(tabulate(extractedPatternList_DF, headers='keys', tablefmt='psql'))

"""# Temporal Pattern Extraction"""

PATTERN_MATCHED_TEMPORAL = {
    "RESPONSE_PATTERN_REGEX" : [],
    "ALTERNATING_PATTERN_REGEX" : [],
    "MULTIEFFECT_PATTERN_REGEX" : [],
    "MULTICAUSE_PATTERN_REGEX" : []
}

# Note:
# ({0}{1}) when formatted with ("a","b") -> (ab)
# ({0}{1}) when formatted with ("a1","b1") -> (a1b1) { this is not a valid regex }
# (({0})({1})) when formatted with ("a1","b1") -> ((a1)(b1)) { this is valid regex as grouped}
# Following uses grouping

PATTERN_REGEX_DIC = {
    "RESPONSE_PATTERN_REGEX" : "[^({0})]*(({0})[^({1})]*({1})[^({0})]*)*",
    "ALTERNATING_PATTERN_REGEX" : "[^({0})({1})]*(({0})[^({0})({1})]*({1})[^({0})({1})]*)*",
    "MULTIEFFECT_PATTERN_REGEX" : "[^({0})({1})]*(({0})[^({0})({1})]*({1})[^({0})]*)*",
    "MULTICAUSE_PATTERN_REGEX" : "[^({0})({1})]*(({0})[^({1})]*({1})[^({0})({1})]*)*"
}

"""#### Extract and Define Parameters """

Alphabets = sorted(list(set(timed_trace[0])))
temporal_patterns_pat_str_count = []

"""#### Begin extraction of temporal properties """

def find_matching_pattern(extracted_pattern: list = [], interested_pattern: str = "RESPONSE_PATTERN_REGEX"):
    global PATTERN_REGEX_DIC, PATTERN_MATCHED_TEMPORAL, Alphabets, ENABLE_DEBUG, ENABLE_STRICT_TEMPORAL_MATCH
    pattern_regex = PATTERN_REGEX_DIC[interested_pattern]

    for alpha_1 in Alphabets:
        for alpha_2 in Alphabets:
            if alpha_1 != alpha_2:

                matching_regex = pattern_regex.format(alpha_1, alpha_2)
                if ENABLE_DEBUG:
                    print("For ", interested_pattern, "   Matching with ", matching_regex)
                _currentPattern = ""
                for _ele in extracted_pattern:
                    _cp = [s.char for s in _ele]
                    _currentPattern = ''.join(_cp)

                    try:
                        if re.fullmatch(matching_regex, _currentPattern):
                            if ENABLE_DEBUG:
                                print("Matched with ", _currentPattern)

                            if ENABLE_STRICT_TEMPORAL_MATCH:
                                if doesStringContains(_currentPattern, alpha_1) and doesStringContains(_currentPattern,
                                                                                                       alpha_2):
                                    PATTERN_MATCHED_TEMPORAL[interested_pattern].append([(alpha_1, alpha_2), _ele])
                            else:
                                PATTERN_MATCHED_TEMPORAL[interested_pattern].append([(alpha_1, alpha_2), _ele])

                    except Exception as e:
                        print(e)
                        print("matching_regex ", matching_regex, "  _currentPattern ", _currentPattern)

def find_all_temporal_patterns(extracted_pattern: list = []):
    global PATTERN_REGEX_DIC
    for key, value in PATTERN_REGEX_DIC.items():
        find_matching_pattern(extracted_pattern, key)


def printAllTemporalPatterns():
    global PATTERN_REGEX_DIC, PATTERN_MATCHED_TEMPORAL, temporal_patterns_pat_str_count, ENABLE_DEBUG
    for key, value in PATTERN_MATCHED_TEMPORAL.items():
        if ENABLE_DEBUG:
            print("For Temporal Pattern : ", key)

        for _pat in value:
            result_pattern = "{0}  -    {1}".format(_pat[0][0],_pat[0][1])

            if ENABLE_DEBUG:
                print(result_pattern, " : -----------")

            _pattern_count = [key, result_pattern] + printFromPatternList([_pat[1]], [], True) # Print the pattern
            temporal_patterns_pat_str_count.append(_pattern_count)

        if ENABLE_DEBUG:
            print("--------")

start = time.time()

find_all_temporal_patterns(extractedPatternList)
time_diff = round(time.time() - start, 2)
print("Temporal patterns of interest extracted")
if ENABLE_STRICT_TEMPORAL_MATCH:
    print("NOTE: Patterns with does not contain SRC -> DES events are IGNORED !! ")

print("Timed Elapsed : ", str(time_diff), " sec")


printAllTemporalPatterns()

temporalPatternList_DF = pd.DataFrame(temporal_patterns_pat_str_count, columns=["Type","FromToEvent","Pattern","Count", "Support", "Confidence"])
temporalPatternList_DF.sort_values(by=["Count"], ascending=False, inplace=True)

print(tabulate(temporalPatternList_DF, headers='keys', tablefmt='psql'))


"""# Provide Probability over time for pattern"""
VARIABILITY_OF_LOOKBACK = True # For Fixed K lookback enter False else True for (1, K)

def getRandomSubtrace():
    global K, timed_trace, trace_size;

    k = random.randint(1, K);

    i_pos = random.randint(0, trace_size - K)
    sub_event = timed_trace[0][i_pos:i_pos + k]
    sub_time = timed_trace[1][i_pos:i_pos + k]

    return (sub_event, sub_time)

subtrace = getRandomSubtrace()
print("Subtrace ", subtrace)

def determine_time_length(sub_sub_trace=[]):
    t_max = -1;
    pos = 0;
    sub_sub_trace_timeShifted = getTimeShift(sub_sub_trace[1])

    global timed_trie
    root = timed_trie

    while pos < len(sub_sub_trace[0]):
        _e, _t = sub_sub_trace[0][pos], sub_sub_trace_timeShifted[pos]
        for _child in root.children:
            if _child.char == _e and _child.t_min <= _t and _child.t_max >= _t:

                if pos == len(sub_sub_trace[0])-1:
                    for ___child in _child.children:
                        t_max = max(___child.t_list)

                root = _child
                break; # break for

        pos += 1

    return t_max;

def get_time_length_based_on_lookback(subTrace, variable_lookback: bool = True):

    global ENABLE_DEBUG
    t_max = 0;
    subTrace_len = len(subTrace[0])

    for _k in list(range(subTrace_len, 0, -1)):
        if variable_lookback == False and _k < subTrace_len:
            break;

        sub_sub_trace = subTrace[0][-_k:]
        sub_sub_trace_time = subTrace[1][-_k:]

        __t_max_sub = determine_time_length((sub_sub_trace, sub_sub_trace_time))
        if ENABLE_DEBUG:
            print("k ", str(_k), "   --- ", __t_max_sub)
        t_max = max(t_max, __t_max_sub)

    return t_max

tp_max = get_time_length_based_on_lookback(subtrace, VARIABILITY_OF_LOOKBACK)
if ENABLE_DEBUG:
    print("T_max window ", tp_max)


time_probability = np.zeros((tp_max, len(Alphabets)))

def evaluate_time_probability_using_count():
    global time_probability
    for index, val in enumerate(time_probability):
        _count_list_total = sum(val)
        if _count_list_total > 0:
            _prob_list = [round(float(e/_count_list_total), 2) for e in val]
            time_probability[index] = _prob_list

def fill_time_probability_mat(root: TrieNode, sub_sub_trace):
    pos = 0;
    sub_sub_trace_timeShifted = getTimeShift(sub_sub_trace[1])
    global time_probability, Alphabets;

    while pos < len(sub_sub_trace[0]):
        _e, _t = sub_sub_trace[0][pos], sub_sub_trace_timeShifted[pos]
        for _child in root.children:
            if _child.char == _e and _child.t_min <= _t and _child.t_max >= _t:
                root = _child
                if pos == len(sub_sub_trace[0]) - 1:
                    # Now fill the
                    # for ___child in _child.children:
                    for time_step in list(range(len(time_probability))):
                        for __child in _child.children:
                            all_time_transitions = __child.t_list
                            _pos_of_char_in_Alphabet = Alphabets.index(__child.char)
                            _count = len(list(filter(lambda _x: _x == time_step+1, all_time_transitions)))
                            time_probability[time_step][_pos_of_char_in_Alphabet] += _count

                break;  # break for

        pos += 1

    return;
#
def fill_time_probability_mat_on_lookBack(root: TrieNode, sub_sub_trace, variable_lookback: bool = True):
    global ENABLE_DEBUG

    subTrace_len = len(sub_sub_trace[0])

    for _k in list(range(subTrace_len, 0, -1)):
        if variable_lookback == False and _k < subTrace_len:
            break;

        sub_sub_trace_event = sub_sub_trace[0][-_k:]
        sub_sub_trace_time = sub_sub_trace[1][-_k:]

        sub_sub_trace_timeEvent = (sub_sub_trace_event, sub_sub_trace_time)

        if ENABLE_DEBUG:
            print("k ", str(_k), "   filling --- ", sub_sub_trace_timeEvent)

        fill_time_probability_mat(root, sub_sub_trace_timeEvent)

    if ENABLE_DEBUG:
        print("Count list ", time_probability)
    evaluate_time_probability_using_count()


start = time.time()
fill_time_probability_mat_on_lookBack(timed_trie, subtrace, VARIABILITY_OF_LOOKBACK)
print("Time Probability Transition created")
time_diff = round(time.time() - start, 2)
print("Timed Elapsed : ", str(time_diff), " sec")

time_probability_DF = pd.DataFrame(time_probability, columns=Alphabets, index= list(range(1, tp_max+1)))
# print(tabulate(time_probability_DF, headers='keys', tablefmt='psql'))

# print(time_probability)


"""#Plot Area Timed_Area graph"""

def plot_time_probability():
    global time_probability, Alphabets

    y = np.vstack(time_probability)
    x = np.arange(len(Alphabets)) # label location
    # barWidth = 0.2
    barWidth = float(1.0/(2*len(Alphabets)))

    fig, ax = plt.subplots()
    ax.set_ylabel('Probability')
    ax.set_xlabel("Time (sec)")
    ax.set_title('Probablity of Event transition over time')

    r = np.arange(len(y.T[0]))
    for idx, val in enumerate(y.T):

        _label = Alphabets[idx]
        plt.bar(r, val, width=barWidth, label=_label)

        # Set position of bar on X axis
        r = [x + barWidth for x in r]

    plt.xticks([r + barWidth for r in range(len(y))], list(range(1, len(time_probability)+1)))
    plt.legend()

    plt.show()

if len(Alphabets) > PLOT_PROBABILITY_ALPBHABET_MAX_LENGTH:
    print("Skipping plotting of probability graph since alphabet size more than {0}".format(PLOT_PROBABILITY_ALPBHABET_MAX_LENGTH))
else:
    plot_time_probability()


"""#Pruned Trie Model"""

def buildGraphPrunedTrie(extracted_pattern) -> TrieNode:
    global global_node_count_pruned
    global_node_count_pruned += 1

    root = TrieNode(global_node_count_pruned, "*")

    for _idx, _val in enumerate(extracted_pattern):
        _temp = root;
        for _ele in _val:
            found = False;
            for _child in _temp.children:
                if _child.char == _ele.char:
                    _temp = _child
                    found = True
                    break;

            if found == False:
                global_node_count_pruned += 1
                newNode = TrieNode(global_node_count_pruned, _ele.char)
                newNode.t_min = _ele.t_min
                newNode.t_max = _ele.t_max
                newNode.t_list = _ele.t_list
                newNode.count = _ele.count
                newNode.prob = _ele.prob
                newNode.prob_pattern = _ele.prob_pattern
                newNode.tree_height = _ele.tree_height
                newNode.is_end = _ele.is_end
                newNode.dropped = _ele.dropped

                _temp.children.append(newNode)
                _temp = newNode

    return root

global_node_count_pruned = 0;
timed_trie_pruned = buildGraphPrunedTrie(extractedPatternList)

print("Timed Trie (pruned version) created.")

"""### Visualize"""

renderTrie(timed_trie_pruned, "timed_trie_pruned_state_diagram", True)
renderTrie(timed_trie_pruned, "timed_trie_pruned", False)

"""### Save results"""
# extractedPatternList_DF.to_csv(
#         "/home/pradeep/UBC/Projects/Data-Mining/TTT - A_star/codebase/timedStateTrie/Result/hexacopter_dominant_properties.csv")
# temporalPatternList_DF.to_csv("/home/pradeep/UBC/Projects/Data-Mining/TTT - A_star/codebase/timedStateTrie/Result/hexacopter_temporal_properties.csv")
